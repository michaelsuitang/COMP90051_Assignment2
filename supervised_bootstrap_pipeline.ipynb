{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77258ad-431f-4fe1-b964-ca7398ce016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, numpy as np, pandas as pd, torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db3cde1-2426-4517-9868-5d6b7663801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     pos_rate   balance\n",
      "Attractive           0.512505  0.012505\n",
      "Mouth_Slightly_Open  0.483428  0.016572\n",
      "Smiling              0.482080  0.017920\n",
      "Wearing_Lipstick     0.472436  0.027564\n",
      "High_Cheekbones      0.455032  0.044968\n"
     ]
    }
   ],
   "source": [
    "# ---------- pick 5 binary attrs (the closer to 0.5, the better for training) ----------\n",
    "\n",
    "csv_path = Path(r\"list_attr_celeba.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert CelebA labels from {-1, 1} to {0, 1} for binary classification.\n",
    "attr_cols = [c for c in df.columns if c != \"image_id\"]\n",
    "for c in attr_cols:\n",
    "    if set(df[c].unique()) <= {-1, 1}:\n",
    "        df[c] = (df[c] == 1).astype(int)\n",
    "\n",
    "# compute the positive-class ratio, then compute how close it is to a perfectly balanced 50/50 split\n",
    "stats = (\n",
    "    df[attr_cols].mean().to_frame(\"pos_rate\")\n",
    "      .assign(balance=lambda d: (d[\"pos_rate\"] - 0.5).abs())\n",
    "      .sort_values(\"balance\"))\n",
    "\n",
    "# Show the 5 most balanced attributes\n",
    "print(stats.head(5))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067df046-9a48-459c-a137-b36be3bb27ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([300, 256])\n",
      "Y shape: torch.Size([300, 5])\n",
      "     image_id  Attractive  Mouth_Slightly_Open  Smiling  Wearing_Lipstick  \\\n",
      "0  000001.jpg           1                    1        1                 1   \n",
      "1  000002.jpg           0                    1        1                 0   \n",
      "2  000003.jpg           0                    0        0                 0   \n",
      "3  000004.jpg           1                    0        0                 1   \n",
      "4  000005.jpg           1                    0        0                 1   \n",
      "\n",
      "   High_Cheekbones  \n",
      "0                1  \n",
      "1                1  \n",
      "2                0  \n",
      "3                0  \n",
      "4                0  \n"
     ]
    }
   ],
   "source": [
    "# model.py will produce two files: features_256.pt and features_filenames.txt\n",
    "\n",
    "## ----------step1. fix 5 binary attrs ----------\n",
    "# export_first_300_features.py (generates features from the first 300 images; just an example to verify the pipeline, can be deleted).\n",
    "\n",
    "X = torch.load(r\"data\\features_256.pt\")               # e.g., torch.Size([300, 256])\n",
    "names = Path(r\"data\\features_filenames.txt\").read_text().splitlines()\n",
    "assert len(names) == X.shape[0], f\"names({len(names)}) != X({X.shape[0]})\"\n",
    "\n",
    "# Read CelebA attributes and convert {-1, 1} to {0, 1}\n",
    "def load_attrs(csv_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    attr_cols = [c for c in df.columns if c != \"image_id\"]\n",
    "    for c in attr_cols:\n",
    "        vals = set(df[c].unique())\n",
    "        if vals <= {-1, 1}:                      # only convert if it's -1/1\n",
    "            df[c] = (df[c] == 1).astype(int)     # -> 0/1\n",
    "    return df, attr_cols\n",
    "\n",
    "df, attr_cols = load_attrs(r\"list_attr_celeba.csv\")\n",
    "\n",
    "# Manually fix the 5 attributes you want to use\n",
    "PICK_ATTRS = [              \n",
    "    \"Attractive\",\n",
    "    \"Mouth_Slightly_Open\",\n",
    "    \"Smiling\",\n",
    "    \"Wearing_Lipstick\",\n",
    "    \"High_Cheekbones\",]\n",
    "\n",
    "# Safety check: columns must exist in the CSV\n",
    "missing = set(PICK_ATTRS) - set(attr_cols)\n",
    "assert not missing, f\"Unknown attrs: {missing}\"\n",
    "\n",
    "# Align the attribute table to the same order as features using the saved filenames\n",
    "dfA = df.set_index(\"image_id\").loc[names].reset_index()\n",
    "\n",
    "# Build the label tensor Y for the 5 tasks (shape: n√ó5). You can also take one column at a time.\n",
    "Y = torch.tensor(dfA[PICK_ATTRS].values, dtype=torch.float32)\n",
    "\n",
    "print(\"X shape:\", X.shape)  # [n, 256]\n",
    "print(\"Y shape:\", Y.shape)  # [n, 5]\n",
    "print(dfA[[\"image_id\"] + PICK_ATTRS].head())  # quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6acfeb-1a35-4c69-95fb-fe809dc8f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step 2: create FeatureDataset  ----------\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, X, y): self.X, self.y = X.float(), y.float().view(-1,1)\n",
    "    def __len__(self): return self.X.size(0)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb2d57ea-c417-4b8b-af9f-33c4e6649993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 3: Supervised bootstrap tuning (lr & betas) ----\n",
    "\n",
    "# Tune hyperparameters using the pre-extracted features X:[n,256] and a binary label y:[n]\n",
    "\n",
    "# Bootstrap splitting in the same style as the unsupervised script (sampling with replacement; out-of-bag as validation)\n",
    "def bootstrap_split_indices(n, B=10, seed=90051):\n",
    "    random.seed(seed)\n",
    "    splits = []\n",
    "    for _ in range(B):\n",
    "        train_idx = [random.randint(0, n - 1) for _ in range(n)]\n",
    "        val_idx = list(set(range(n)) - set(train_idx))  # Out-of-bag (OOB)\n",
    "        if len(val_idx) == 0:                           # Fallback for extreme cases\n",
    "            val_idx = list(range(0, n, max(1, n // 5)))\n",
    "        splits.append((train_idx, val_idx))\n",
    "    return splits\n",
    "\n",
    "# A very lightweight linear / two-layer MLP classifier (same as step 4)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim=256, hidden=None):\n",
    "        super().__init__()\n",
    "        if hidden is None:\n",
    "            self.net = nn.Linear(in_dim, 1)  # Logistic regression\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden), nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def _train_epochs(model, dl_tr, epochs, optimizer, device):\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"sum\")  # Use 'sum' reduction, same as in the unsupervised script\n",
    "    model.to(device)\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def _eval_loss(model, dl_va, device):\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    for xb, yb in dl_va:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        total += loss_fn(model(xb), yb).item()\n",
    "    return total\n",
    "\n",
    "def tune_supervised_with_bootstrap(\n",
    "    X, y, device=\"cpu\", B=3, epochs=8, batch_size=64,\n",
    "    lr_candidates=(1e-2, 1e-3, 5e-4),\n",
    "    betas_candidates=((0.9, 0.999), (0.9, 0.99), (0.5, 0.999)),\n",
    "    hidden=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns: (best_lr, best_betas)\n",
    "    Note: This is only for hyperparameter tuning. In each bootstrap fold it trains a temporary small \n",
    "    classifier and selects hyperparameters based on the mean OOB (out-of-bag) validation loss.\n",
    "    It does not perform a final train on the full training set or any test-set evaluation.\n",
    "    \"\"\"\n",
    "    n = X.size(0)\n",
    "    num_workers = 0  # more stable for windows\n",
    "\n",
    "    # ------- Step 1: Fix betas; grid-search the learning rate (lr) -------\n",
    "    betas_fixed = (0.9, 0.999)\n",
    "    best_lr, best_loss = None, float(\"inf\")\n",
    "    for lr in lr_candidates:\n",
    "        losses = []\n",
    "        for train_ids, val_ids in bootstrap_split_indices(n, B):\n",
    "            ds_tr = FeatureDataset(X[train_ids], y[train_ids])\n",
    "            ds_va = FeatureDataset(X[val_ids],  y[val_ids])\n",
    "            dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True,\n",
    "                               num_workers=num_workers, pin_memory=(device==\"cuda\"))\n",
    "            dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False,\n",
    "                               num_workers=num_workers, pin_memory=(device==\"cuda\"))\n",
    "            model = Classifier(in_dim=256, hidden=hidden)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas_fixed)\n",
    "            _train_epochs(model, dl_tr, epochs, optimizer, device)\n",
    "            val_loss = _eval_loss(model, dl_va, device)\n",
    "            losses.append(val_loss)\n",
    "\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        print(f\"[Step 1] lr={lr:.1e}, betas={betas_fixed} -> mean OOB val loss={avg_loss:.4f}\")\n",
    "        if avg_loss < best_loss:\n",
    "            best_lr, best_loss = lr, avg_loss\n",
    "    print(f\"Best lr={best_lr} (mean OOB loss={best_loss:.4f})\")\n",
    "\n",
    "    # ------- Step 2: Fix the best lr; grid-search betas -------\n",
    "    best_betas, best_loss = None, float(\"inf\")\n",
    "    for betas in betas_candidates:\n",
    "        losses = []\n",
    "        for train_ids, val_ids in bootstrap_split_indices(n, B):\n",
    "            ds_tr = FeatureDataset(X[train_ids], y[train_ids])\n",
    "            ds_va = FeatureDataset(X[val_ids],  y[val_ids])\n",
    "            dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True,\n",
    "                               num_workers=num_workers, pin_memory=(device==\"cuda\"))\n",
    "            dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False,\n",
    "                               num_workers=num_workers, pin_memory=(device==\"cuda\"))\n",
    "            model = Classifier(in_dim=256, hidden=hidden)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=best_lr, betas=betas)\n",
    "            _train_epochs(model, dl_tr, epochs, optimizer, device)\n",
    "            val_loss = _eval_loss(model, dl_va, device)\n",
    "            losses.append(val_loss)\n",
    "\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        print(f\"[Step 2] betas={betas}, lr={best_lr:.1e} -> mean OOB val loss={avg_loss:.4f}\")\n",
    "        if avg_loss < best_loss:\n",
    "            best_betas, best_loss = betas, avg_loss\n",
    "\n",
    "    print(f\"\\nFinal best params: lr={best_lr}, betas={best_betas}, mean OOB val loss={best_loss:.4f}\")\n",
    "    return best_lr, best_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458d77d8-b921-4401-8aba-f9cafc4b7d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] lr=1.0e-02, betas=(0.9, 0.999) -> mean OOB val loss=75.5357\n",
      "[Step 1] lr=1.0e-03, betas=(0.9, 0.999) -> mean OOB val loss=75.5162\n",
      "[Step 1] lr=5.0e-04, betas=(0.9, 0.999) -> mean OOB val loss=75.5593\n",
      "Best lr=0.001 (mean OOB loss=75.5162)\n",
      "[Step 2] betas=(0.9, 0.999), lr=1.0e-03 -> mean OOB val loss=75.5155\n",
      "[Step 2] betas=(0.9, 0.99), lr=1.0e-03 -> mean OOB val loss=75.5262\n",
      "[Step 2] betas=(0.5, 0.999), lr=1.0e-03 -> mean OOB val loss=75.5363\n",
      "\n",
      "Final best params: lr=0.001, betas=(0.9, 0.999), mean OOB val loss=75.5155\n",
      "Chosen: 0.001 (0.9, 0.999)\n"
     ]
    }
   ],
   "source": [
    "# sample about how to use (using export_first_300_features.py), only to ensure the code runs, can be deleted.\n",
    "# Assume you already have:\n",
    "# X: torch.Size([n, 256])  generated by `export_first_300_features.py`\n",
    "# dfA: an attribute table aligned with `X` (including `\"image_id\"` and your 5 selected attributes)\n",
    "\n",
    "import torch\n",
    "task = \"Smiling\"  # pick any one attribute\n",
    "y_task = torch.tensor(dfA[task].values, dtype=torch.float32)\n",
    "\n",
    "best_lr, best_betas = tune_supervised_with_bootstrap(\n",
    "    X, y_task,\n",
    "    device=\"cpu\",      # set `device=\"cuda\"` if a GPU is available\n",
    "    B=3, epochs=8, batch_size=64,\n",
    "    lr_candidates=(1e-2, 1e-3, 5e-4),\n",
    "    betas_candidates=((0.9,0.999),(0.9,0.99),(0.5,0.999)),\n",
    "    hidden=None        # if you plan to use a two-layer FC as in step 4, set `hidden=128` here (for tuning only)\n",
    ")\n",
    "print(\"Chosen:\", best_lr, best_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe98048-86a2-4f27-b248-1f29ccc4447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step 4: 2-layer fully connected model ----------\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
